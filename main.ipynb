{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizers CPP\n",
    "\n",
    "In this note book I want to compare the time to tokenize certain texts with common tokenizers compared to my own!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alex/miniconda3/envs/tokenizers/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import tokenizers_cpp\n",
    "from min_bpe.basic import BasicTokenizer\n",
    "import time\n",
    "from datasets import load_dataset\n",
    "import random\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train BPE Tokenizer\n",
    "\n",
    "I use MSMARCO to train my own BPE tokenizer. I'm going to be using other text for inference so I'm going to mix training, validation and test data from MSMARCO to get the most amount of data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msmarco = load_dataset(\"ms_marco\", \"v2.1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_data = []\n",
    "\n",
    "for phase in [\"test\"]:\n",
    "    dataset = msmarco[phase]\n",
    "\n",
    "    for entry in dataset:\n",
    "        for answer in entry[\"answers\"]:\n",
    "            text_data.append(answer)\n",
    "        \n",
    "        for passage in entry[\"passages\"][\"passage_text\"]:\n",
    "            text_data.append(passage)\n",
    "        \n",
    "        text_data.append(entry[\"query\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.shuffle(text_data)\n",
    "with open(\"training_text.txt\", 'w') as f:\n",
    "    for line in text_data:\n",
    "        f.write(f\"{line}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train BPETokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_text = Path(\"training_text.txt\").read_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 50257\n",
    "tok = tokenizers_cpp.BPETokenizer(vocab_size)\n",
    "basic_tok = BasicTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = time.time()\n",
    "tok.train(training_text)\n",
    "e = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Tokenizers CPP took {e - s} time!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = time.time()\n",
    "basic_tok.train(training_text, vocab_size)\n",
    "e = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Basic Tokenizer took {e - s} time!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tokenizers",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
